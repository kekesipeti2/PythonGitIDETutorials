# -*- coding: utf-8 -*-
"""CVS_HW_2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/szemenyeim/bbb6a1e73446d7bd00b0a596b798b9d9/cvs_hw.ipynb

# **Computer Vision Systems Homework**

**Dear fellow engineers!**

The nice people of Sim City would like to welcome you in their humble town, which is built entirely from environmentally friendly, recyclable triangles and pixels. In their most recent town meeting, the people decided that it was time for Sim City to adopt self-driving vehicles, since apparently, they are all the craze in the real world. 

However, the people are worried that accidents caused by these cars would significantly increase the computational resources used by Sim City due to an increased respawn rate. Therefore, the good people of Sim City need **you** to develop an algorithm that can reliably detect and track all relevant objects around the vehicles.

## **Basics**

As mentioned before, your task is to develop an algorithm that can perform detection, tracking and segmentation tasks on synthetic images created using the [CARLA simulator](https://carla.org/). There are seven videos in total, all annotated with the correct results for most tasks. 

Your job is to develop algorithms that attempt to find the correct results for all images and then compare the results of your method with the correct answers. From this comparison you can calculate the average error or the average accuracy of your method. Since object detection in autonomous driving settings is a real-time task, you also need to evaluate the speed of your algorithm.

## The setting

The videos in the provided dataset are saved frame-by-frame, meaning for every frame your have access to the following data:

* **Inputs:**
    1. RGB frame (located in "carla_images/rgb/[video_index]/[frame_number].jpg" )
    2. Depth frame (located in "carla_images/depth/[video_index]/[frame_number].png" ) This frame is the same size az the RGB frame, but contains the distance of the pixels from the camera in cm.
    3. The example below contains the camera matrix values that you can use for converting [x, y, depth] points to 3D coordinates. Look up the pinhole camera equations below for help.
* **Outputs:**
    1. Freespace image (located in "carla_images/seg/[video_index]/fs_[frame_number].png" ) This is a binary image, were all the road pixels are 1, the rest is 0.
    2. Semantic segmentation image (located in "carla_images/seg/[video_index]/[frame_number].png" ) A color image, showing the semantic label of each image. To use this with deep neural networks, first it has to be converted into a format, where the pixels are the semantic labels, not colors. This means the pixel values have to correspond to the specific label values that the given neural network uses.
    3. Bounding box annotations (located in "carla_images/annotation/[video_index]/[frame_number]_.txt" ) This is a text file, where every row contains a detectable object (detectable objects are 0:cars, 1:trucks, 2:motorcycles, 3:bicycles, 4:pedestrians). The values of the detection are as follows: [class_index, BB_x, BB_y, BB_w, BB_h, BB3D_x1, BB3D_x2, BB3D_y1, BB3D_y2, BB3D_z1, BB3D_z2]. Below, you can find an example using these values.
    4. The self-motion of the vehicle (located in "carla_images/annotation/[video_index]/[frame_number]_.npy" ) A binary file containing the matrix describing the 6DOF rigid transformation of the car relative to its position in the previous frame.

## Tasks

Using the datasets provided, you have to perform the following tasks:

1. **2D Object Detection:** This task can be done with or without neural networks. You simply have to detect all 5 object types, provide accurate bounding boxes and class labels. You can initially use a pre-trained neural net, but for really great results it is recommended that you fine-tune using your own data (don't forget to do a train-validation split). You can also try and enhance the network performance by adding depth as an extra channel (note: this is difficult).

2. **3D Object Detection:** This tasks has to be done **without** neural nets. Use the results of the 2D tasks (if not completed yet, you can use the correct outputs in the meanwhile) to compute the object locations in 3D relative to the car. Also try and track the objects on multiple frames and try to determine their velocity.

3. **Semantic Segmentation:** Use a neural network to perform semantic segmentation on the RGB images. You can initially use a pre-trained network, but it is also recommended that you fine-tune using your own data, just like in the first task. Similarly, you can add the depth image as an extra feature either as an input channel, or as an input to the last layer (note: this is considerably easier to do here than in the first case).

4. **Freespace Segmentation:** Use traditional methods (meaning you **can't** use neural nets here) to find pixels belonging to the road. Note, that the car is always in a fixed position and orientation relative to the ground plane, so this task is relatively easy to do using simply 3D geometry (simplified RANSAC-like methods).

**Important**

Each of the tasks described here is pretty hard, meaning you won't be able to do all of them perfectly. So your job is to do as many of these as you can, as well as you can.

## Evaluation

Your job is also to evaluate the reliability of your own algorithms. Here are some tips:

1. If your train/fine-tune methods, make sure you always use a train-validation split. There are 9 videos in total, so 7-2 or 6-3 is reasonable. The individual videos are under different conditions (day/night, sunshine/rain, etc.), so make your both sets include a wide range of conditions.
2. To measure the accuracy of your predicted 2D or 3D bounding boxes, the IoU (Intersection over Union) metric is recommended.
3. A detection is accurate if the IoU is high enough and a class label is correct. To measure the performance of 2D detection, there are several useful metrics: Precision, Recall, F1-Score and mAP (mean Average Precision).
4. For the segmentation-like tasks, pixel accuracy is the simplest metric (the % of pixels classified correctly), but it is not very good. I recommend that you use the IoU metric (first IoU values are computed for every class, and then averaged).
5. For evaluating the speed of the methods, you can use the average time for processing a single frame. If you use tricks, where you only do certain computations for the first frame, then you can exclude the first frame from the average.

## **Submission and Presentation**

In the retake week (23.-27. May) there will be a special session, where the teams **give a short (10 minutes) presentation** about the work they completed. The presentation should focus on the method used and the results achieved. The teams will also have to **submit their code and a short (10-15 page) report** by the end of the retake week. 

The offered grade will be decided by a panel of lecturers involved in the course after hearing the presentation and inspecting the code and the report. Note, that in the case of a very low-quality submission, we may decide not to give an offered grade (this means you will have to take the final exam).

Truly high quality homeworks can be carried on as individual research subjects for the Students' Scientific Conference (TDK).

## Rules

Here are some important rules and guidelines you have to follow:

*   This work is to be done in groups of 4 people. You can do it with less if you feel confident, but not more.
*   Forming/finding a group is your job. Once you have one, 1 person from the group should write me a message on teams with the names and Neptun codes of the members.
*   You can opt out of the homework later. In this case you will need to take the midterm exam. If you signed up for the homework and then decide to opt out, let me know ASAP.

## So, how should we do this?

So, how can you do this homework, especially if you haven't done things like this before? Here are a few tips:

### Environment

For development IDE the easiest is to just use Google Colab. To do this you just have to solve the homework inside this notebook. This is the simplest solution, although it has one drawback: the Colab notebook has limited debugging capabilities.

If you want something more powerful, I recommend the [PyCharm](https://www.jetbrains.com/pycharm/) IDE, which is a free and pretty powerful Python development tool.

If you are planning to use PyCharm on Windows, you need to install a Python distribution, since Windows still doesn't come with one (it's 20 effing 22, Microsoft!). I recommend [Anaconda](https://www.anaconda.com/distribution/). Make sure you use Python 3.x and not 2.7.

[Here's a tutorial on how to set it up.](https://www.youtube.com/watch?v=e53lRPmWrMI)

### Collaboration within the team

Since I would strongly discourage teams to collaborate physically in the current situation, I'd like to recommend some methods for remote collaboration.

* First of all, use git or a similar version control tool to handle multiple team members working on the same project. 
* I strongly recommend creating a private repository for your homework on [Github](https://github.com/) (since you can add exactly 3 collaborators - including you that's a 4-person team). There, you can also create issues and other nice-to-have features to track your development. Getting some experience with version control is an absolute must for any engineer anyways.

Here's a tutorial for git for those who never used something like this before.

To use git from a GUI, I recommend [SmartGit](https://www.syntevo.com/smartgit/) or [Git Extensions](http://gitextensions.github.io/).

**ProTip:** If you use a Colab notebook, make sure to clear the output cells (especially figures and images) before you commit. Otherwise you'll litter in your repository.

[Here is an introduction to git](https://www.freecodecamp.org/news/learn-the-basics-of-git-in-under-10-minutes-da548267cc91/)

### Further resources

As this course uses the Python language, I highly recommend that you also use this language to do your homework. Here are some resources to get you started (Note: the practicals will also provide you with many of the basics, but they are still a few weeks away):

[Python tutorials](https://docs.python.org/3/tutorial/)

OpenCV: For basic computer vision and image processing

[OpenCV tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)

PyTorch: For deep learning

[PyTorch tutorials](https://pytorch.org/tutorials/)

# Solution
## Download dataset
"""

# Homework dataset
!wget http://deeplearning.iit.bme.hu/Carla_Videos.zip
!unzip -qq Carla_Videos.zip
!rm Carla_Videos.zip

"""## Folder example

Get all subfolders in a directory

```
import os
myFolderList = [f.path for f in os.scandir(path) if f.is_dir()]
```

Get all files with extension in a directory

```
import glob
import re

def sorted_nicely( l ):
    ''' Sort the given iterable in the way that humans expect.'''
    convert = lambda text: int(text) if text.isdigit() else text
    alphanum_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]
    return sorted(l, key = alphanum_key)

names = sorted_nicely(glob.glob1(path, "*.extension"))
```

## Pinhole camera equations

This small example shows the relationship between the world coordinates $[x, y, z]$ and the projected image coordinates $[u, v]$ depending on the values in the camera matrix $A$.

$$u = f_x\frac{x}{z} + p_x$$

$$v = f_y\frac{-y}{z} + p_y$$

$$A = \begin{bmatrix} f_x & 0 & p_x \\ 0 & f_y & p_y \\ 0 & 0 & 1 \end{bmatrix}$$

### Display the first images
"""

# Commented out IPython magic to ensure Python compatibility.
import pickle
import cv2
import matplotlib.pyplot as plt
import numpy as np
#This way it doesn't try to open a window un the GUI - works in python notebook
# %matplotlib inline

cam_mtx = np.array([
    [358.5, 0.0,   512.0],
    [0.0,   358.5, 256.0],
    [0.0,   0.0,   1.0],
    
])

colors = [(0,0,255), (0,255,0), (255,0,255), (0,255,255), (255,0,0)]
classNames = ["Car", "Truck", "Motorcycle", "Bicycle", "Pedestrian"]

'''
The key element of this dictionary is the semantic ID of a class you can use with
neural networks, while the value is the RGB color used in the images.
'''
semSegClasses = {  
     0: [0, 0, 0],         # None 
     1: [70, 70, 70],      # Buildings 
     2: [190, 153, 153],   # Fences 
     3: [72, 0, 90],       # Other 
     4: [220, 20, 60],     # Pedestrians 
     5: [153, 153, 153],   # Poles 
     6: [157, 234, 50],    # RoadLines 
     7: [128, 64, 128],    # Roads 
     8: [244, 35, 232],    # Sidewalks 
     9: [107, 142, 35],    # Vegetation 
     10: [0, 0, 255],      # Vehicles 
     11: [102, 102, 156],  # Walls 
     12: [220, 220, 0]     # TrafficSigns 
 } 

def drawBBs(BBs, img):
    H, W = img.shape[:2]
    for BB in BBs:
        u = BB[1]*W
        v = BB[2]*H
        w = BB[3]*W
        h = BB[4]*H
        c = int(BB[0])
        x_min = BB[5]
        x_max = BB[6]
        y_min = BB[5]
        y_max = BB[6]
        z_min = BB[5]
        z_max = BB[6]
        s = (int(u - w // 2), int(v - h // 2))
        e = (int(u + w // 2), int(v + h // 2))
        cv2.rectangle(img, s, e, colors[c], 1)
        tl = (s[0], s[1]+15)
        bl = (s[0], e[1]-5)
        cv2.putText(img,classNames[c],tl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.75,colors[c])
        #coords = "(%.2f, %.2f, %.2f, %.2f, %.2f, %.2f)" % (x_min, x_max, y_min, y_max, z_min, z_max)
        #cv2.putText(img,coords,bl,cv2.FONT_HERSHEY_COMPLEX_SMALL,0.65,colors[c])
    
    return img

# Read images
img = cv2.imread("carla_images/rgb/000/00.jpg")
depth = cv2.imread("carla_images/depth/000/00.png", -1)
seg = cv2.imread("carla_images/seg/000/00.png")
fs = cv2.imread("carla_images/seg/000/fs_00.png", -1)

# Read annotations
labels = np.loadtxt("carla_images/annotation/000/00_.txt")
pos = np.load("carla_images/annotation/000/00_.npy").reshape((4,4))

# Visualization
img = drawBBs(labels, img)
img_rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)
seg_rgb = cv2.cvtColor(seg,cv2.COLOR_BGR2RGB)

# Figure with subplots
plt.figure(figsize=(30,15))
plt.subplot(2,2,1)
plt.imshow(img_rgb)
plt.subplot(2,2,2)
plt.imshow(depth,cmap='gray')
plt.subplot(2,2,3)
plt.imshow(seg_rgb)
plt.subplot(2,2,4)
plt.imshow(fs,cmap='gray')

"""# Your Work"""

